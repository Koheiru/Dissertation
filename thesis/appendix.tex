\appendix
%%% Оформление заголовков приложений ближе к ГОСТ:
\setlength{\midchapskip}{20pt}
\renewcommand*{\afterchapternum}{\par\nobreak\vskip \midchapskip}
\renewcommand\thechapter{\Asbuk{chapter}} % Чтобы приложения русскими буквами нумеровались


%==============================================================================
%                Свидетельство о регистрации программы для ЭВМ
%==============================================================================
\chapter{Свидетельство о регистрации программы для ЭВМ} \label{appendix:programm_registration}
\begin{figure}[ht]
    \framebox{\includegraphics[width=0.9\textwidth]{program_registration_no2015660380}}
\end{figure}


%==============================================================================
%                Дополнительный материал
%==============================================================================
\chapter{Дополнительный материал} \label{appendix:math}

%==============================================================================
\section{Вывод оригинальной функции активации}  \label{appendix:activation_function}

Несмотря на то, что в работе~\cite{EmelyanovYaroslavsky1990} рассматривается стохастическая импульсная модель нейрона, автор так же отмечает, что при рассмотрении \socalled пачки импульсов нейрон можно рассматривать как детерминированный пороговый элемент и усреднённая частота возникновения импульсов будет определяться равенством: $$\text{U}^\text{М} - \text{П}_{0}^\text{Д}(\tau) \cdot \text{П}_{1}^\text{Д}(Q^\Sigma) = 0 ,$$ где $\text{U}^\text{М}$ --- медленный потенциал нейрона, отражающий вклад возбуждающего и тормозного воздействия других нейронов, а так же влияние медленно изменяющихся пороговых величин самого нейрона, $\text{П}_{0}^\text{Д}(\tau)$ --- функция динамического порога, зависящая от прошедшего с момента предыдущего импульса времени $\tau$ и изображённая \onfigure~\ref{fig:ia_thresholds}а, $\text{П}_{1}^\text{Д}(Q^\Sigma)$ --- модулирующая динамический порог функция, зависящая от \socalled самочувствия нейронов сети $Q^\Sigma$ и изображённая \onfigure~\ref{fig:ia_thresholds}б.

\IncludeFigure{ia_thresholds}{Определённые в работе~\cite{EmelyanovYaroslavsky1990}: а) функция динамического порога $\text{П}_{0}^\text{Д}$, зависящая от межспайкового интервала $\tau$; б) модулирующая динамический порог функция $\text{П}_{1}^\text{Д}$, зависящая от параметра самочувствия нейронов $Q^\Sigma$.}

Для удобства введём величину $\bar{\text{U}}^\text{М} = \text{U}^\text{М} / \text{П}_{1}^\text{Д}(Q^\Sigma)$ и примем во внимание, что временной интервал между возникновением импульсов в пачке несоизмеримо мал в сравнении с временем, необходимым для ощутимого изменения медленного потенциала $\text{U}^\text{М}$. Тогда усреднённая частота генерации импульсов $y$ будет определяться выражением: 
$$ y = \dfrac{1}{\tau} = \dfrac{1}{{\text{П}_{0}^\text{Д}}^{-1}(\bar{\text{U}}^\text{М})}.$$

Однако явно использовать обратную функцию ${\text{П}_{0}^\text{Д}}^{-1}(\bar{\text{U}}^\text{М})$ невозможно, т.к. функция $\text{П}_{0}^\text{Д}(\tau)$ не обладает свойством монотонности при $\tau \in \left[ \tau_{1}^{\text{н}}; \tau_{2}^{\text{н}} \right]$. В то же время, основываясь на том, что нас интересуют усреднённые частотные величины, мы можем без потери качественных свойств преобразовать функцию $\text{П}_{0}^\text{Д}(\tau)$ к монотонному виду $f(y)$, изображенному \onfigure~\ref{fig:ia_thresholds_fixed}.

\IncludeFigure{ia_thresholds_fixed}{Функция $f(y)$, являющаяся результатом приведения функции $\text{П}_{0}^\text{Д}(\tau)$, изображённой \onfigure~\ref{fig:ia_thresholds}а, к функционально эквивалентному монотонному виду.}

Далее необходимо формализовать функцию $f(y)$, основываясь на её качественном графическом представлении. Для этого зададим её на области  $D(f) = \left[0; 1\right]$ в виде кусочно-гладкой функции, изображенной \onfigure~\ref{fig:ia_activation_function}:
$$
    f(y) = 
    \begin{cases}
        f_{1}(y) = \dfrac{2,6}{1 + e^{\displaystyle -(y - 0,1935) \cdot 120}}   &, y \in \left[0; y_{12}\right]    \\
        f_{2}(y) = k_{12} + \dfrac{z_{12}}{1 - y}                               &, y \in \left(y_{12}; 1\right]    \\
    \end{cases}
$$
где $y_{12} = 0,245$ и коэффициенты были подобраны так, чтобы качественно повторять форму изображённой \onfigure~\ref{fig:ia_thresholds_fixed} кривой, а так же функция в целом была непрерывно-дифференцируемой, \ie $f_{1}(y_{12}) = f_{2}(y_{12})$ и $f^{\prime}_{1}(y_{12}) = f^{\prime}_{2}(y_{12})$:
\begin{align*}
    &z_{12} = 177,84 \cdot \sigma\left(6,18\right) \cdot \left(1 - \sigma\left(6,18\right)\right) \approx 0,3667, \\
    &k_{12} = 2,6 \cdot \sigma\left(6,18\right) - z_{12} / 0,755 \approx 2,1089,
\end{align*}
где $\sigma$ --- логистическая функция.

Обратная функция $f^{-1}(y)$ выводится аналитически и имеет вид:
$$
    f^{-1}(u) = 
    \begin{cases}
        f_{0}^{-1}(u) = 0                                                               &, u < u_{01}      \\  
        f_{1}^{-1}(u) = 0,1935 + \dfrac{1}{120} \ln\left( \dfrac{u}{2,6 - u} \right)    &, u_{01} \le u \le u_{12}  \\
        f_{2}^{-1}(u) = 1 - \dfrac{z_{12}}{x - k_{12}}                                  &, u > u_{12}    \\
    \end{cases}
$$
где $u_{01} = 2,6 \cdot \sigma\left(-23,22\right) \approx 0$ и $u_{12} = 2,6 \cdot \sigma\left(6,18\right) \approx 2,5946$.

\IncludeFigure{ia_activation_function}{Вид: а) кусочно-гладкой функции $f(y)$; б) функций $f_{1}(y)$ и $f_{2}(y)$, которые определяют $f(y)$ на интервалах $\left[0; 0,245\right]$ и $\left(0,245; 1\right]$ соответственно. }

Таким образом, используемая в диссертационной работе \textit{оригинальная функция активации} определяется как $g_{s}(u) := f^{-1}(u)$.

Стоит отметить, что в опубликованных в рамках исследовательской деятельности работах \cite{Prostov2015-OMNN,Prostov2015-MEPhI,Prostov2015-ESU} была использована следующая функция активации:
$$
    \hat{g}_{s}(u) = 
    \begin{cases}
        0                                       &, u < 0            \\  
        \dfrac{1}{5,2 + 0,23 \ln(2,6/u - 1)}    &, 0 \le u < 2,6    \\
        \dfrac{1}{1 + 0,35 / (u - 2,46)}        &, u \ge 2,6        \\
    \end{cases}
$$
которая имеет более сложную аналитическую форму, а так же хоть и малые, но разрывы в местах смены функций, что затрудняет её аналитическое исследование. Тем не менее, с точки зрения поиска и оценки качественных свойств нейрона как динамической системы можно говорить об их эквивалентности, что было подтвержденно в численных экспериментах. Для наглядности \onfigure~\ref{fig:ia_sfunction_compare} изображены оба варианта оригинальной функции активации в сравнительной форме.

\IncludeFigure{ia_sfunction_compare}{Вид функции активации ${g}_{s}$, использованной в диссертационной работе, и функции $\hat{g}_{s}$, использованной в части опубликованных работ, а так же график их разности: $\Delta(u) = \left|{g}_{s}(u) - \hat{g}_{s}(u)\right|$.}

%==============================================================================
\section{Численное сравнение конечно-разностных схем}  \label{appendix:differential_schemes_compare}

Для эмпирической оценки рассмотренных в работе конечно-разностных схем были проведены численные эксперименты с моделью \textit{простого микроансамбля}. Для этого система~\eqref{eq:simple_microensemble_model} была приведена к конечно-разностному виду с использованием параметров $\mu = 0,75$, $\scalar{p} = 1$, $\alpha = 3,5$ и $\theta = 1$ следующими методами:
\begin{itemize}
    \item явным методом Эйлера с шагами дискретизации $h=0,1$ и $h=0,01$;
    \item методом Эйлера с пересчётом с шагом дискретизации $h=0,1$;
    \item методом Рунге-Кутты 4-го порядка с шагом дискретизации $h=0,1$.
\end{itemize}

Результаты моделирования в условиях изменяющегося во времени внешнего возбуждения $\scalar{i} = \vector{w}^{\top} \vector{x}$ продемонстрированы \onfigure~\ref{fig:ds_sigm_simulation} (вариант с \textit{сигмоидальной} функцией активации) и \onfigure~\ref{fig:ds_origin_simulation} (вариант с \textit{оригинальной} функцией активации), на которых погрешности $\Delta\scalar{y}$ и $\Delta\scalar{u}$ вычислены относительно результатов метода Рунге-Кутты. Видно, что явный метод Эйлера с шагом $h=0,01$ показывает приемлемый результат, хотя наилучшим с точки зрения баланса точности и вычислительных затрат является метод Эйлера с пересчётом.

\IncludeFigure[p]{ds_sigm_simulation}{\todo{Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата ...}}

\IncludeFigure[p]{ds_origin_simulation}{\todo{Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата ...}}

%==============================================================================
\section{Постановка задачи анализа независимых компонент}  \label{appendix:ica_description}

Рассмотрим постановку задачи анализа независимых компонент в стационарном случае. Для этого предположим существование \textit{скрытого} случайного вектора $\vector{s}_{m \times 1}$, который соответствует ненаблюдаемому источнику данных, а также предположим, что для него характерно следующее:
\begin{itemize}
    \item компоненты статистически независимы: $\forall\, i,j\ P(\vector{s}_{i}, \vector{s}_{j}) = P(\vector{s}_{i}) P(\vector{s}_{j})$;
    \item компоненты центрированы: $\forall\, i\ \expectation{\vector{s}_{i}} = 0$;
    \item нормальное распределение имеет не более одной компоненты (в противном случае, согласно центральной предельной теореме, их разделение будет проблематично): $\nexists\, i,j:\ \vector{s}_{i} \sim N\left(0, \sigma_{i}\right), \vector{s}_{j} \sim N\left(0, \sigma_{j}\right)$.
\end{itemize}
В этом случае вектор $\vector{s}$ называется \textbf{вектором независимых компонент}, а его элементы $\vector{s}_{i}$ --- \textbf{независимыми компонентами}. Далее предположим, что \textit{наблюдаемый} случайный вектор $\vector{x}_{n \times 1}$ является результатом смешения компонент вектора $\vector{s}$, \ie: $$\vector{x} = f_{\theta}(\vector{s}) + \vector{\varepsilon},$$ где $f_{\theta}$ --- неизвестная функция смешивания, зависящая от вектора параметров $\theta$, и $\varepsilon \sim N\left(0, \text{diag}\left(\Sigma\right)\right)$ --- нормально распределённый случайный вектор с нулевым математическим ожиданием и дисперсией $\Sigma$, который отражает наличие шума в наблюдаемых данных. Тогда решением задачи \acr{ICA} будет являться нахождение по выборке наблюдаемого случайного вектора $\vector{x}$ обратной функции $F_{\theta}$, такой что: $$\vector{s} = F_{\theta}(\vector{x}).$$ 
В итоге, применение обратной функции $F_{\theta}$ к любой реализации вектора $\vector{x}$ даст значения соответствующих ему независимых компонент $\vector{s}_{i}$. Это означает, что мы получим способ проецирования наблюдаемых векторов в пространство, в котором компоненты векторов обладают свойством статистической независимости, что является следствием определения вектора независимых компонент.

В линейном случае формулировка задачи приобретает вид: $\vector{x} = \matrix{A} \vector{s} + \varepsilon,$ где $\matrix{A}_{n \times m}$ --- матрица смешивания независимых компонент вектора $\vector{s}$. Решением задачи в этом случае будет нахождение обратной матрицы $\matrix{W}$ (а в случае, когда $m > n$, псевдо обратной матрицы), такой что: $\vector{s} = \matrix{W} \vector{x}.$


Для примера, \onfigure~\ref{fig:ica_compare_with_pca} изображен результат применения анализа независимых компонент и метода главных компонент к набору данных, \todo{полученному...} Можно отметить, что визуально проекция на пространство независимых компонент выглядит более интересной, чем проекция на пространство главных компонент.

\IncludeFigure{ica_compare_with_pca}{\todo{Сравнение результата ICA и PCA}.}


%==============================================================================
%                Дополнительные изображения
%==============================================================================
\chapter{Дополнительные изображения} \label{appendix:figures}

\IncludeCenteredFigure{model_sigm_configurations}{Конфигурации графиков решения системы~\eqref{eq:simple_microensemble_model} с \textit{сигмоидальной} функцией активации в плоскости параметров $\scalar{i}$ и $\alpha$.}
\IncludeCenteredFigure{model_origin_configurations}{Конфигурации графиков решения системы~\eqref{eq:simple_microensemble_model} с \textit{оригинальной} функцией активации в плоскости параметров $\scalar{i}$ и $\alpha$.}
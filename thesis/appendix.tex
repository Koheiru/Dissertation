\appendix
%%% Оформление заголовков приложений ближе к ГОСТ:
\setlength{\midchapskip}{20pt}
\renewcommand*{\afterchapternum}{\par\nobreak\vskip \midchapskip}
\renewcommand\thechapter{\Asbuk{chapter}} % Чтобы приложения русскими буквами нумеровались


%==============================================================================
%                Свидетельство о регистрации программы для ЭВМ
%==============================================================================
\chapter{Свидетельство о регистрации программы для ЭВМ} \label{appendix:programm_registration}
\begin{figure}[ht]
    \framebox{\includegraphics[width=0.9\textwidth]{program_registration_no2015660380}}
\end{figure}


%==============================================================================
%                Дополнительный материал
%==============================================================================
\chapter{Дополнительный материал} \label{appendix:math}

%==============================================================================
\section{Вывод оригинальной функции активации}  \label{appendix:activation_function}

Несмотря на то, что в работе~\cite{EmelyanovYaroslavsky1990} рассматривается стохастическая импульсная модель нейрона, автор так же отмечает, что при рассмотрении \socalled пачки импульсов нейрон можно рассматривать как детерминированный пороговый элемент и усреднённая частота возникновения импульсов будет определяться равенством: $$\text{U}^\text{М} - \text{П}_{0}^\text{Д}(\tau) \cdot \text{П}_{1}^\text{Д}(Q^\Sigma) = 0 ,$$ где $\text{U}^\text{М}$ --- медленный потенциал нейрона, отражающий вклад возбуждающего и тормозного воздействия других нейронов, а так же влияние медленно изменяющихся пороговых величин самого нейрона, $\text{П}_{0}^\text{Д}(\tau)$ --- функция динамического порога, зависящая от прошедшего с момента предыдущего импульса времени $\tau$ и изображённая \onfigure~\ref{fig:ia_thresholds}а, $\text{П}_{1}^\text{Д}(Q^\Sigma)$ --- модулирующая динамический порог функция, зависящая от \socalled самочувствия нейронов сети $Q^\Sigma$ и изображённая \onfigure~\ref{fig:ia_thresholds}б.

\IncludeFigure{ia_thresholds}{Определённые в работе~\cite{EmelyanovYaroslavsky1990}: а) функция динамического порога $\text{П}_{0}^\text{Д}$, зависящая от межспайкового интервала $\tau$; б) модулирующая динамический порог функция $\text{П}_{1}^\text{Д}$, зависящая от параметра самочувствия нейронов $Q^\Sigma$.}

Для удобства введём величину $\bar{\text{U}}^\text{М} = \text{U}^\text{М} / \text{П}_{1}^\text{Д}(Q^\Sigma)$ и примем во внимание, что временной интервал между возникновением импульсов в пачке несоизмеримо мал в сравнении с временем, необходимым для ощутимого изменения медленного потенциала $\text{U}^\text{М}$. Тогда усреднённая частота генерации импульсов $y$ будет определяться выражением: 
$$ y = \dfrac{1}{\tau} = \dfrac{1}{{\text{П}_{0}^\text{Д}}^{-1}(\bar{\text{U}}^\text{М})}.$$

Однако явно использовать обратную функцию ${\text{П}_{0}^\text{Д}}^{-1}(\bar{\text{U}}^\text{М})$ невозможно, т.к. функция $\text{П}_{0}^\text{Д}(\tau)$ не обладает свойством монотонности при $\tau \in \left[ \tau_{1}^{\text{н}}; \tau_{2}^{\text{н}} \right]$. В то же время, основываясь на том, что нас интересуют усреднённые частотные величины, мы можем без потери качественных свойств преобразовать функцию $\text{П}_{0}^\text{Д}(\tau)$ к монотонному виду $f(y)$, изображенному \onfigure~\ref{fig:ia_thresholds_fixed}.

\IncludeFigure{ia_thresholds_fixed}{Функция $f(y)$, являющаяся результатом приведения функции $\text{П}_{0}^\text{Д}(\tau)$, изображённой \onfigure~\ref{fig:ia_thresholds}а, к функционально эквивалентному монотонному виду.}

Далее необходимо формализовать функцию $f(y)$, основываясь на её качественном графическом представлении. Для этого зададим её на области  $D(f) = \left[0; 1\right]$ в виде кусочно-гладкой функции, изображенной \onfigure~\ref{fig:ia_activation_function}:
$$
    f(y) = 
    \begin{cases}
        f_{1}(y) = \dfrac{2,6}{1 + e^{\displaystyle -(y - 0,1935) \cdot 120}}   &, y \in \left[0; y_{12}\right]    \\
        f_{2}(y) = k_{12} + \dfrac{z_{12}}{1 - y}                               &, y \in \left(y_{12}; 1\right]    \\
    \end{cases}
$$
где $y_{12} = 0,245$ и коэффициенты были подобраны так, чтобы качественно повторять форму изображённой \onfigure~\ref{fig:ia_thresholds_fixed} кривой, а так же функция в целом была непрерывно-дифференцируемой, \ie $f_{1}(y_{12}) = f_{2}(y_{12})$ и $f^{\prime}_{1}(y_{12}) = f^{\prime}_{2}(y_{12})$:
\begin{align*}
    &z_{12} = 177,84 \cdot \sigma\left(6,18\right) \cdot \left(1 - \sigma\left(6,18\right)\right) \approx 0,3667, \\
    &k_{12} = 2,6 \cdot \sigma\left(6,18\right) - z_{12} / 0,755 \approx 2,1089,
\end{align*}
где $\sigma$ --- логистическая функция.

\IncludeFigure{ia_activation_function}{Вид: а) кусочно-гладкой функции $f(y)$; б) функций $f_{1}(y)$ и $f_{2}(y)$, которые определяют $f(y)$ на интервалах $\left[0; 0,245\right]$ и $\left(0,245; 1\right]$ соответственно. }

Обратная функция $f^{-1}(u)$ выводится аналитически и имеет вид:
$$
    f^{-1}(u) = 
    \begin{cases}
        f_{0}^{-1}(u) = 0                                                               &, u < u_{01}      \\  
        f_{1}^{-1}(u) = 0,1935 + \dfrac{1}{120} \ln\left( \dfrac{u}{2,6 - u} \right)    &, u_{01} \le u \le u_{12}  \\
        f_{2}^{-1}(u) = 1 - \dfrac{z_{12}}{x - k_{12}}                                  &, u > u_{12}    \\
    \end{cases}
$$
где $u_{01} = 2,6 \cdot \sigma\left(-23,22\right) \approx 0$ и $u_{12} = 2,6 \cdot \sigma\left(6,18\right) \approx 2,5946$.

\begin{Definition*}
    \textit{Оригинальная функция активации}, используемая в диссертационной работе, определяется как $g_{s}(u) := f^{-1}(u)$, а обратная ей функция, соответственно, как $g^{-1}_{s}(u) := f(y)$.
\end{Definition*}

Стоит отметить, что в опубликованных в рамках исследовательской деятельности работах \cite{Prostov2015-OMNN,Prostov2015-MEPhI,Prostov2015-ESU} была использована следующая функция активации:
$$
    \hat{g}_{s}(u) = 
    \begin{cases}
        0                                       &, u < 0            \\  
        \dfrac{1}{5,2 + 0,23 \ln(2,6/u - 1)}    &, 0 \le u < 2,6    \\
        \dfrac{1}{1 + 0,35 / (u - 2,46)}        &, u \ge 2,6        \\
    \end{cases}
$$
которая имеет более сложную аналитическую форму, а так же хоть и малые, но разрывы в местах смены функций, что затрудняет её аналитическое исследование. Тем не менее, с точки зрения поиска и оценки качественных свойств нейрона как динамической системы можно говорить об их эквивалентности, что было подтвержденно в численных экспериментах. Для наглядности \onfigure~\ref{fig:ia_sfunction_compare} изображены оба варианта оригинальной функции активации в сравнительной форме.

\IncludeFigure{ia_sfunction_compare}{Вид функции активации ${g}_{s}$, использованной в диссертационной работе, и функции $\hat{g}_{s}$, использованной в части опубликованных работ, а так же график их разности: $\Delta(u) = \left|{g}_{s}(u) - \hat{g}_{s}(u)\right|$.}

%==============================================================================
\section{Условия существования точек перегиба в кривой решений модели}  \label{appendix:origin_roots_existance}

В~\autoref{subsection:analysis_origin} для системы~\eqref{eq:simple_microensemble_model} с \textit{сигмоидальной} функцией активации были найдены корни, соответствующие точкам перегиба в графике решений~\eqref{eq:functional_y}:
\begin{equation}
    \nonumber
    \begin{cases}
        \scalar{x}_{1,2} = d \pm \sqrt{d^{2} - 1},                              & \scalar{x} \in \left[x_{12}; x_{01}\right) \\
        \scalar{y}_{3\phantom{,4}}   = 1 - \sqrt{z_{12} \mu \theta / \alpha},   & \scalar{y} \in \left(y_{12}; 1\right] \\
    \end{cases}
\end{equation}
где величина $d = \left( 156 \mu \theta / \alpha - 1\right)$ введена для удобства дальнейших выкладок. Так же напомним, что в соответствии с определением системы~\eqref{eq:simple_microensemble_model} $\scalar{y} \in [0;1]$, $\mu \in (0;1)$, $\theta \in \specialset{R}^{+}$ и $\alpha \ge 0$, откуда так же следует, что при сколь угодно большом, но ограниченном значении параметра $\alpha$ выражение $\mu \theta / \alpha > 0$.

Условия существования этих корней, помимо неотрицательности выражений под знаком корня, включают в себя и учёт области их определения, что связано с кусочно-заданной формой функции активации. Отметим, что эти условия удобно определять относительно величины $\mu \theta / \alpha$, т.к. в~\autoref{subsection:analysis_origin} они используются в процессе бифуркационного анализа как по параметру $\theta$, так и по параметру $\alpha$. 

\begin{Statement*}
    \textit{Область существования корня} $\scalar{y}_{3} = 1 - \sqrt{z_{12} \mu \theta / \alpha}$ на интервале $\left(y_{12}; 1\right]$ определяется условием: $$ \mu \theta / \alpha < \left(1 - y_{12}\right)^{2} / z_{12}, $$ что приближённо можно оценить как: $\mu \theta / \alpha < 1,56 + o(1)$.
\end{Statement*}
\begin{Proof}
    Заметим, что значение выражения под знаком корня в определении переменной $\scalar{y}_{3}$ всегда больше 0, а значение самой переменной $\scalar{y}_{3}$ при этом всегда меньше 1, т.к. $z_{12} > 0$ и $\mu \theta / \alpha > 0$. Поэтому результат очевидным образом следует из граничного условия $\scalar{y}_{3} > y_{12}$ при подстановке в него определения переменной $\scalar{y}_{3}$.
\end{Proof}

\begin{Statement*}
    \textit{Область существования корня} $\scalar{x}_{1} = d + \sqrt{d^{2} - 1}$ на интервале $\left[x_{12}; x_{01}\right)$ в виде вещественного и отличного от корня $\scalar{x}_{2}$ значения определяется условием: $$ 1 / 78 < \mu \theta / \alpha < (x_{01} + 1)^{2} / (312 x_{01}), $$ что приближённо можно оценить как: $1,3 \cdot 10^{-2} + o(1) < \mu \theta / \alpha < 3,9 \cdot 10^{7} + o(1)$.
\end{Statement*}
\begin{Proof}
    Чтобы значение корня $\scalar{x}_{1}$ было вещественным и отличным от $\scalar{x}_{2}$, выражение под знаком корня в его определении должно быть положительным, \ie должно выполняться условие $d^{2} - 1 > 0$. Однако сразу отметим, что его можно свести к условию $d > 1$, т.к. при $\mu \theta / \alpha > 0$ неравенство $d < -1$ решений иметь не будет. Кроме того, область существования корня ограничена интервалом $\left[x_{12}; x_{01}\right)$, поэтому рассмотрим решение неравенства $\scalar{x}_{1} \ge x_{12}$, подставив определение переменной $\scalar{x}_{1}$ и учитывая ограничение $d > 1$:
    \begin{equation}
        \nonumber
        \begin{cases}
            d + \sqrt{d^{2} - 1} &\ge x_{12} \\
            d &> 1 \\
            \end{cases}
        \Rightarrow
        \begin{sqcases}
            \begin{cases}
                d^{2} - 1 &\ge \left( x_{12} - d \right)^{2} \\
                x_{12} - d &\ge 0 \\
                d &> 1 \\
            \end{cases} \\
            \begin{cases}
                d^{2} - 1 &\ge 0 \\
                x_{12} - d &< 0 \\
                d &> 1 \\
            \end{cases}
        \end{sqcases}
        \Rightarrow
        \begin{sqcases}
            \begin{cases}
                d &\ge (x_{12}^{2} + 1) / (2 x_{12}) \\
                d &\le x_{12} \\
                d &> 1 \\
            \end{cases} \\
            \begin{cases}
                d &\ge 1 \\
                d &> x_{12} \\
                d &> 1 \\
            \end{cases}
        \end{sqcases}.
    \end{equation}
    Однако $x_{12} \approx 2,1 \cdot 10^{-3}$, поэтому первая система решений иметь не будет, а вторая сводится к интервалу $d > 1$. Теперь рассмотрим неравенство $\scalar{x}_{1} < x_{01}$: \\
    \begin{equation}
        \nonumber
        \begin{cases}
            d + \sqrt{d^{2} - 1} &< x_{01} \\
            d &> 1 \\
            \end{cases}
        \Rightarrow
            \begin{cases}
            d^{2} - 1 &< \left( x_{01} - d \right)^{2} \\
            x_{01} - d &> 0 \\
            d &> 1 \\
            \end{cases}
        \Rightarrow
        \begin{cases}
            d &< (x_{01}^{2} + 1) / (2 x_{01}) \\
            d &< x_{01} \\
            d &> 1 \\
        \end{cases},
    \end{equation}
    что сводится к интервалу $1 < d < (x_{01}^{2} + 1) / (2 x_{01})$, т.к. $x_{01} \approx 1,2 \cdot 10^{10}$. 
    
    Таким образом, объединяя решения обеих неравенств и подставляя определение переменной $d$, мы получим условие существования корня $\scalar{x}_{1}$ в виде неравенства: 
    $ 1 / 78 < \mu \theta / \alpha < (x_{01} + 1)^{2} / (312 x_{01}) $ --- ч.т.д.
\end{Proof}

\begin{Statement*}
    \textit{Область существования корня} $\scalar{x}_{2} = d - \sqrt{d^{2} - 1}$ на интервале $\left[x_{12}; x_{01}\right)$ в виде вещественного и отличного от корня $\scalar{x}_{1}$ значения определяется условием: $$ 1 / 78 < \mu \theta / \alpha \le (x_{12} + 1)^{2} / (312 x_{12}), $$ что приближённо можно оценить как: $1,3 \cdot 10^{-2} + o(1) < \mu \theta / \alpha \le 1,56 + o(1)$.
\end{Statement*}
\begin{Proof}
    В соответствии с аналогичным доказательством для корня $\scalar{x}_{1}$, область существования корня $\scalar{x}_{2}$ так же ограничена интервалом $\left[x_{12}; x_{01}\right)$ с условием, что $d > 1$. Поэтому рассмотрим решение неравенства $\scalar{x}_{2} \ge x_{12}$, подставив определение переменной $\scalar{x}_{2}$ и учитывая дополнительное условие:
    \begin{equation}
        \nonumber
        \begin{cases}
            d - \sqrt{d^{2} - 1} &\ge x_{12} \\
            d &> 1 \\
        \end{cases}
        \Rightarrow
        \begin{cases}
            d^{2} - 1 &\le \left( d - x_{12} \right)^{2} \\
            d - x_{12} &\ge 0 \\
            d &> 1 \\
        \end{cases}
        \Rightarrow
        \begin{cases}
            d &\le (x_{12}^{2} + 1) / (2 x_{12}) \\
            d &\ge x_{12} \\
            d &> 1 \\
        \end{cases},
    \end{equation}
    что можно свести к интервалу $1 < d \le (x_{12}^{2} + 1) / (2 x_{12})$ при известном значении $x_{12}$. Теперь рассмотрим неравенство $\scalar{x}_{2} < x_{01}$:
    \begin{equation}
        \nonumber
        \begin{cases}
            d - \sqrt{d^{2} - 1} &< x_{01} \\
            d &> 1 \\
        \end{cases}
        \Rightarrow
        \begin{sqcases}
            \begin{cases}
                d^{2} - 1 &> \left( d - x_{01} \right)^{2} \\
                d - x_{01} &\ge 0 \\
                d &> 1 \\
            \end{cases} \\
            \begin{cases}
                d^{2} - 1 &\ge 0 \\
                d - x_{01} &< 0 \\
                d &> 1 \\
            \end{cases}
        \end{sqcases}
        \Rightarrow
        \begin{sqcases}
            \begin{cases}
                d &> (x_{01}^{2} + 1) / (2 x_{01}) \\
                d &\ge x_{01} \\
                d &> 1 \\
            \end{cases} \\
            \begin{cases}
                d &< x_{01} \\
                d &> 1 \\
            \end{cases}
        \end{sqcases},
    \end{equation}
    где при известном значении $x_{01}$ первая система сводится к интервалу $d \ge x_{01}$ и вторая --- к интервалу $1 < d < x_{01}$. В результате их объединения решением станет интервал $d > 1$.
    
    Таким образом, объединяя решения обеих неравенств и подставляя определение переменной $d$, мы получим условие существования корня $\scalar{x}_{2}$ в виде неравенства: 
    $ 1 / 78 < \mu \theta / \alpha \le (x_{12} + 1)^{2} / (312 x_{12}) $ --- ч.т.д.
\end{Proof}

%==============================================================================
\section{Численное сравнение конечно-разностных схем}  \label{appendix:differential_schemes_compare}

Для эмпирической оценки рассмотренных в работе конечно-разностных схем были проведены численные эксперименты с моделью \textit{простого микроансамбля}. Для этого система~\eqref{eq:simple_microensemble_model} была приведена к конечно-разностному виду с использованием параметров $\mu = 0,75$, $\scalar{p} = 1$, $\alpha = 3,5$ и $\theta = 1$ следующими методами:
\begin{itemize}
    \item явным методом Эйлера с шагами дискретизации $h=0,1$ и $h=0,01$;
    \item методом Эйлера с пересчётом с шагом дискретизации $h=0,1$;
    \item методом Рунге-Кутты 4-го порядка с шагом дискретизации $h=0,1$.
\end{itemize}

Результаты моделирования в условиях изменяющегося во времени внешнего возбуждения $\scalar{i} = \vector{w}^{\top} \vector{x}$ продемонстрированы \onfigure~\ref{fig:ds_sigm_simulation} (вариант с \textit{сигмоидальной} функцией активации) и \onfigure~\ref{fig:ds_origin_simulation} (вариант с \textit{оригинальной} функцией активации), на которых погрешности $\Delta\scalar{y}$ и $\Delta\scalar{u}$ вычислены относительно результатов метода Рунге-Кутты. Видно, что явный метод Эйлера с шагом $h=0,01$ показывает приемлемый результат, хотя наилучшим с точки зрения баланса точности и вычислительных затрат является метод Эйлера с пересчётом.

\IncludeFigure[p]{ds_sigm_simulation}{\todo{Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата ...}}

\IncludeFigure[p]{ds_origin_simulation}{\todo{Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата Сравнение результата ...}}

%%==============================================================================
%\section{Постановка задачи анализа независимых компонент}  \label{appendix:ica_description}
%
%Рассмотрим постановку задачи анализа независимых компонент в стационарном случае. Для этого предположим существование \textit{скрытого} случайного вектора $\vector{s}_{m \times 1}$, который соответствует ненаблюдаемому источнику данных, а также предположим, что для него характерно следующее:
%\begin{itemize}
%    \item компоненты статистически независимы: $\forall\, i,j\ P(\vector{s}_{i}, \vector{s}_{j}) = P(\vector{s}_{i}) P(\vector{s}_{j})$;
%    \item компоненты центрированы: $\forall\, i\ \expectation{\vector{s}_{i}} = 0$;
%    \item нормальное распределение имеет не более одной компоненты (в противном случае, согласно центральной предельной теореме, их разделение будет проблематично): $\nexists\, i,j:\ \vector{s}_{i} \sim N\left(0, \sigma_{i}\right), \vector{s}_{j} \sim N\left(0, \sigma_{j}\right)$.
%\end{itemize}
%В этом случае вектор $\vector{s}$ называется \textbf{вектором независимых компонент}, а его элементы $\vector{s}_{i}$ --- \textbf{независимыми компонентами}. Далее предположим, что \textit{наблюдаемый} случайный вектор $\vector{x}_{n \times 1}$ является результатом смешения компонент вектора $\vector{s}$, \ie: $$\vector{x} = f_{\theta}(\vector{s}) + \vector{\varepsilon},$$ где $f_{\theta}$ --- неизвестная функция смешивания, зависящая от вектора параметров $\theta$, и $\varepsilon \sim N\left(0, \text{diag}\left(\Sigma\right)\right)$ --- нормально распределённый случайный вектор с нулевым математическим ожиданием и дисперсией $\Sigma$, который отражает наличие шума в наблюдаемых данных. Тогда решением задачи \acr{ICA} будет являться нахождение по выборке наблюдаемого случайного вектора $\vector{x}$ обратной функции $F_{\theta}$, такой что: $$\vector{s} = F_{\theta}(\vector{x}).$$ 
%В итоге, применение обратной функции $F_{\theta}$ к любой реализации вектора $\vector{x}$ даст значения соответствующих ему независимых компонент $\vector{s}_{i}$. Это означает, что мы получим способ проецирования наблюдаемых векторов в пространство, в котором компоненты векторов обладают свойством статистической независимости, что является следствием определения вектора независимых компонент.
%
%В линейном случае формулировка задачи приобретает вид: $\vector{x} = \matrix{A} \vector{s} + \varepsilon,$ где $\matrix{A}_{n \times m}$ --- матрица смешивания независимых компонент вектора $\vector{s}$. Решением задачи в этом случае будет нахождение обратной матрицы $\matrix{W}$ (а в случае, когда $m > n$, псевдо обратной матрицы), такой что: $\vector{s} = \matrix{W} \vector{x}.$
%
%
%Для примера, \onfigure~\ref{fig:ica_compare_with_pca} изображен результат применения анализа независимых компонент и метода главных компонент к набору данных, \todo{полученному...} Можно отметить, что визуально проекция на пространство независимых компонент выглядит более интересной, чем проекция на пространство главных компонент.
%
%\IncludeFigure{ica_compare_with_pca}{\todo{Сравнение результата ICA и PCA}.}


%==============================================================================
%                Дополнительные изображения
%==============================================================================
\chapter{Дополнительные изображения} \label{appendix:figures}

\IncludeCenteredFigure{model_sigm_configurations}{Конфигурации графиков решения системы~\eqref{eq:simple_microensemble_model} с \textit{сигмоидальной} функцией активации в плоскости параметров $\scalar{i}$ и $\alpha$.}
\IncludeCenteredFigure{model_origin_configurations}{Конфигурации графиков решения системы~\eqref{eq:simple_microensemble_model} с \textit{оригинальной} функцией активации в плоскости параметров $\scalar{i}$ и $\alpha$.}
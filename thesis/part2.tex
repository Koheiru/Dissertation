\chapter{Модель гистерезисного нейрона} \label{chapter:neuron}

Основой для анализируемой в данной главе модели нейрона послужила монография~\cite{EmelyanovYaroslavsky1990}, в которой рассматривается подход к построению искусственной \acr{NN}, потенциально обладающей рядом интеллектуальных свойств. Особенность работы заключается в том, что автор предлагает оригинальную модель нейрона и показывает через численные и умозрительные эксперименты, что благодаря ряду локальных свойств совокупность нейронов способна демонстрировать нетривиальное поведение. Однако предложенная нейросетевая модель носит ярко выраженный незаконченный характер и кроме того, преследует целью биологическую правдоподобность, что так же отразилось на модели.

Данная диссертационная работа направлена на потенциальную прикладную значимость получаемой \acr{NN}, в частности, при решении задачи контекстно-зависимого распознавания образов. Поэтому предложенный в работе~\cite{EmelyanovYaroslavsky1990} подход был в значительной мере переосмыслен и в результате выработана собственная нейросетевая модель: она не содержит биологически обоснованных переменных и является \socalled \textit{Firing-Rate} моделью~\cite{Dayan2001}, \ie выходным параметром нейрона является усреднённая по времени частота генерации импульсов нейроном (далее, частота), вместо \socalled стохастической \textit{Integrate-And-Fire} модели~\cite{Dayan2001}, в которой моделирование происходит на уровне отдельных импульсов нейрона. Так же присутствует и ряд других изменений, однако полезные в решении нашей задачи идеи были сохранены: использование эффекта гистерезиса и применение совокупности нейронов в виде элементарных логических единиц сети.

%==============================================================================
%                          Формальное определение модели
%==============================================================================
\section{Формальное определение} \label{section:neuron_model}

Формальная модель нейрона выглядит следующим образом:
\begin{equation}
	\label{eq:single_neuron_model}
    \begin{cases}
	    d\scalar{u}/d\scalar{t} &= \vector{x}^\top \vector{w} - \scalar{p} - \mu \scalar{u}, \\
        \scalar{\nu}            &= f\left( \scalar{u}, \theta \right), \\
    \end{cases}
\end{equation}
где $\scalar{u} \in \specialset{R}$ --- потенциал нейрона, $\vector{x} \in \specialset{R}^{N}$ -- входной вектор нейрона (входной сигнал), $\vector{w} \in \specialset{R}^{N}$ --- вектор весовых коэффициентов связей нейрона,  $\scalar{p}$ --- внутренний порог нейрона, $\mu$ --- константа, характеризующая скорость убывания потенциала нейрона, $\scalar{\nu} \in \left[0;1\right]$ --- частота нейрона (выходной сигнал), $\theta$ --- управляющий параметр сети, функциональный смысл которого будет рассмотрен в следующих главах.

Функция активации $f: \specialset{R} \to \left[0;1\right]$ отражает нелинейное преобразование потенциала нейрона $\scalar{u}$ в его выходную частоту $\scalar{\nu}$. В данной главе будут рассмотрены две функции активации, изображённые \onfigure~\ref{img:model_activation_functions}:
\begin{itemize}
	\item оригинальная функция $s$, полученная на основе функции динамического порога нейрона $\text{П}^\text{Д}$ из работы~\cite{EmelyanovYaroslavsky1990}, но отличающаяся от неё, т.к. функция $\text{П}^\text{Д}$ отражает работу на уровне отдельный импульсов и кроме того, она не была задана автором аналитически:
		\begin{equation}
			s(\scalar{u}, \theta) = 
		\end{equation}
	\item широко распространённая в области машинного обучения сигмоидальная функция $\sigma$:
		\begin{equation}
			\sigma(\scalar{u}, \theta) = 
		\end{equation}
\end{itemize}

%Функция $s$ была получена на основе функции динамического порога нейрона из работы~\cite{EmelyanovYaroslavsky1990} (), но отличается от неё: во-первых, 

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{model_activation_functions}}
    \caption{Вид функции активации: а) \inquotes{оригинальной} функции $s(u)$; б) сигмоидальной функции $\sigma(s)$.}
    \label{img:model_activation_functions}
\end{figure}

\begin{figure}[ht]
	\makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{model_extern_dynamic_threshold}}
	\caption{Вид функции динамического порога нейрона $\text{П}^\text{Д}$ из работы~\cite{EmelyanovYaroslavsky1990}. \todo{В приложение?..}}
	\label{img:model_extern_dynamic_threshold}
\end{figure}

%==============================================================================
%                       Анализ точек равновесия модели
%==============================================================================
\section{Качественный анализ} \label{section:neuron_equilibrium}


%==============================================================================
%                       Характерные параметры модели
%==============================================================================
\section{Характерные свойства и динамика} \label{section:neuron_parameters}


%==============================================================================
%               Моделирование простейшей нейронной сети
%==============================================================================
\section{Моделирование простейших нейронных структур} \label{section:neuron_modeling}

Для проверки принципиальной работоспособности предлагаемой модели далее будут рассмотрены две задачи: задача анализа независимых компонент (\acr{ICA}) и задача построения нейросетевого детерминированного конечного автомата (\acr{FSM}). Для их решения будут смоделированы простые нейронные сети с применением микроансамблей в качестве элементарных элементов. При этом для решения первой задачи параметры модели будут соответствовать <<мягкому>> режиму работы, чтобы продемонстрировать\todo{...}  При решении второй задачи параметры модели будут соответствовать <<жёсткому>> режиму, демонстрируя\todo{...}

\todo{Может быть в этой главе оставить ICA, а FSM перенести в следующую?.. скорее нет, чем да...}

%==============================================================================
\subsection{Решение задачи анализа независимых компонент}

Задача анализа независимых компонент, более подробная постановка которой изложена в Приложении~\ref{appendix:ica_description}, имеет важное прикладное значение, будучи связанной с решением таких проблем, как слепое разделение сигналов и снижение размерности данных~\cite{Hyvarinen2004}. Мы рассмотрим решение частной задачи \acr{ICA} --- <<F{\"o}ldiak bars>>~\cite{Foldiak1990}, которая часто используется в нейросетевой области для первичной проверки соответствующих моделей.

Входные данные представляют собой бинарные изображения размером $8\,\times\,8$ пикселей, сгенерированные с использованием 16-ти независимых компонент: 8-ми горизонтальных и 8-ми вертикальных линий, каждая из которых появлятся на изображении с вероятностью $1/8$ --- независимые компоненты и примеры получаемых на их основе изображений представлены \onfigure~\ref{img:ica_patterns}. Решение же задачи заключается, во-первых, в нахождении в процессе обучения независимых компонент, использованных при генерации изображений, и во-вторых, в разложении в процессе штатной работы входных данных по найденным независимым компонентам. Другими словами, в результате обучения каждой независимой компоненте необходимо поставить в соответствие некоторый нейрон, активность которого будет отражать наличие этой компоненты во входном изображении.

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{ica_patterns}}
    \caption{Бинарные изображения размером 8х8 пикселей: а) набор из 16-ти независимых компонент; б) примеры входных паттернов.}
    \label{img:ica_patterns}
\end{figure}

Стоит отметить, что данная постановка задачи соответствует классу нелинейных задач, т.к. в точках пересечения горизонтальных и вертикальных линий происходит не обычная операция сложения значений $(x + y)$, а сложение значений по модулю: $(x + y) \bmod 1$ ---, что вносит нелинейные искажения в складываемые изображения линий.

\subsubsection{Модель нейронной сети}

Для решения данной задачи была реализована однослойная нейронная сеть, состоящая из 16-ти нейронов, что отражает наши априорные знания о числе ожидаемых независимых компонент в обрабатываемых данных. Формальная модель \acr{NN} имеет вид:
\begin{equation}
    \nonumber
    \begin{cases}
        \dot{\vector{u}} &= \left(\alpha \matrix{I} - \matrix{V} \right)^{\top} \vector{y} + \matrix{W}^{\top} \vector{x} - \mu \vector{u},\\
        \vector{y}       &= f(\vector{u}),
    \end{cases}
\end{equation}
где $\vector{x}_{64 \times 1}$ --- входной вектор, представляющий собой линеаризованное по столбцам входное изображение, $\matrix{W}_{64 \times 16}$ --- как и ранее, матрица, отражающая возбуждающие связи от входного вектора к элементами сети, $\matrix{V}_{16 \times 16}$ --- матрица, отражающая тормозные связи между самими элементами сети. В соответствии с работой~\cite{Fyfe2007}, использование взаимных тормозных связей, обучаемых хэббо-подобным правилом, будет приводить к взаимной декорреляции выходов \acr{NN}. В качестве активационной функции была применена оригинальная функция $s$ (ХХХ) и использованы следующие значения параметров: $\mu = 0,75$, $\theta = 1$, $\alpha = (N - 1) \cdot \omega = 10 \cdot 0,25 = 2,5$ и $p = 1,01$.

Для обучения как возбуждающих, так и тормозных связей использовался один из вариантов STDP-правила~\cite{Izhikevich2003} (\todo{ЗАМЕНИТЬ НА ОБЫЧНОЕ ХЭББОВСКОЕ ПРАВИЛО}):
\begin{equation}
    \nonumber
    \begin{cases}
        \Delta_{ji} &= 
        \begin{cases}
            \vector{y}_{j} \vector{y}_{i} \left( \dfrac{A_{E}}{1/\tau_{E} + \vector{y}_{i}} + \dfrac{A_{I}}{1/\tau_{I} + \vector{y}_{i}}\right), &\text{ если } i \ne j \\
            0, &\text{ иначе } \\
        \end{cases} \\
        \vector{w}_{ji} &= \max \left( 0, \vector{w}_{ji} + \beta_{\vector{w}} \Delta_{ji} \right), \\
        \vector{v}_{ji} &= \max \left( 0, \vector{v}_{ji} - \beta_{\vector{v}} \Delta_{ji} \right), \\
    \end{cases}
\end{equation}
где $A_{E} = 1,01$, $A_{I} = -0,58$, $\tau_{E} = 5$, $\tau_{I} = 38$, $\beta_{\vector{w}} = 0,001$, $\beta_{\vector{v}} = 0,01$, а функция $\max$ обеспечивает неотрицательность элементов матриц $\matrix{W}$ и $\matrix{V}$. Кроме того, после каждой итерации выполнялась нормализация весовых матриц по столбцам, так чтобы $\forall i\ \sum_{j}\vector{w}_{ji} = w^{total}$, где $w^{total} = 2,7$, и $\forall i\ \sum_{j}\vector{v}_{ji} = v^{total}$, где $v^{total} = 5,4$.

\subsubsection{Результаты численных экспериментов}

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{ica_learning_weights}}
    \caption{\todo{Изменение векторов-столбцов возбуждающей матрицы $\matrix{W}$ в процессе обучения...}}
    \label{img:ica_learning_weights}
\end{figure}

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{ica_learning_progress}}
    \caption{\todo{Количество найденных уникальных компонент в процессе обучения...}}
    \label{img:ica_learning_progress}
\end{figure}

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{ica_modeling_result}}
    \caption{\todo{Результат моделирования...}}
    \label{img:ica_modeling_result}
\end{figure}


%==============================================================================
\subsection{Построение нейросетевого конечного автомата}

\todo{Вводная часть...}

\subsubsection{Модель нейронного конечного автомата}

Определим детерминированный конечный автомат (\acr{FSM})~\cite{Hopcroft2008} как формальную математическую структуру $M = \left( \usualset{A}, \usualset{S}, s_{0}, \delta \right)$, где $\usualset{A}$ --- конечное множество входных символов (входной алфавит), $\usualset{S}$ --- конечное множество состояний автомата, $s_{0} \in \usualset{S}$ --- начальное состояние автомата, $\delta\!: \usualset{S}\!\times\!\usualset{A} \to \usualset{S}$ --- функция перехода, отображающая упорядоченную пару элементов (состояние автомата и входной символ) в новое состояние автомата. Если мы обозначим через $q \in \usualset{S}$ текущее состояние автомата, которое в начальный момент времени совпадает с $s_{0}$, то функционирование автомата заключается в последовательном считывании символов $a_{i}$ из входной последовательности $\left\{ a_{0} a_{1} \ldots | a_{j} \in \usualset{A} \right\}$ и переходе в новое состояние: $q = \delta \left( q, a_{i} \right)$. Стоит отметить, что в структуру $M$ часто добавляют множество \socalled заключительных состояний $\usualset{F} \subseteq \usualset{S}$ для верификации считанной входной последовательности, однако в данной работе в этом нет необходимости.

Построение нейросетевой модели по заданной структуре \acr{FSM} можно разбить на две части: построение топологии сети и подбор весовых коэффициентов связей сети. Для задания топологии в первую очередь необходимо каждому элементу всех множеств структуры $M$ поставить в соответствие свой нейрон $n_{i}$: обозначим множество $\left\{ n_{k} | k = \usualset{A} \right\}$, соответствующее множеству $\usualset{A}$, как множество \inquotes{входных} нейронов, множество $\left\{ n_{k} | k = \usualset{S} \right\}$, соответствующее множеству $\usualset{S}$, как множество нейронов \inquotes{состояний} и множество $\left\{ n_{k} | k = D(\delta) \right\}$, соответствующее множеству $S\!\times\!A$ (исключая элементы, для которых отображение $\delta$ не определено), как множество нейронов \inquotes{переходов}. Таким образом, сеть будет состоять из $N = \left|\usualset{A}\right| + \left|\usualset{S}\right| + \left|D(\delta)\right|$ нейронов.

Далее каждому переходу, заданному как элемент множества $\{ \left( s_{i}, s_{j}, a_{k} \right) | \exists \delta\!: s_{i}\!\times\!a_{k} \to s_{j} \}$, необходимо поставить в соответствие набор связей между нейронами сети. Учтём, что в любой момент времени может считываться только один входной символ и должен быть активен только один нейрон \inquotes{состояния} (исключая периоды смены текущего состояния \acr{FSM}, которые связаны с инертностью системы). Поэтому переход из одного состояния в другое является результатом взаимодействия тех нейронов, которые непосредственно соответствуют данному переходому в структуре $M$, а активность остальных должна быть пренебрежимо мала. В результате такой локализации вычислений нейронные связи в сети можно представить в виде типизированного набора, соответствующего каждому переходу в \acr{FSM}: $\beta_{st}$ --- двунаправленная тормозная связь между $n_{s_{i}}$ и $n_{s_{j}}$, $\beta_{tr}$ --- тормозная связь от $n_{s_{i}\!\times\!a_{k}}$ к $n_{a_{k}}$, $\gamma_{st}$ --- возбуждающая связь от $n_{s_{i}}$ к $n_{s_{i}\!\times\!a_{k}}$, $\gamma_{tr}$ -- возбуждающая связь от $n_{s_{i}\!\times\!a_{k}}$ к $n_{s_{j}}$, $\gamma_{in}$ --- возбуждающая связь от $n_{a_{k}}$ к $n_{s_{i}\!\times\!a_{k}}$. \todo{Таким образом, матрица связей \acr{NN} будет имеет следующий вид: ...}

Значения весовых коэффициентов связей подбираются путём решения системы неравенст, описывающей требуемое от элементов сети поведение. Для произвольного перехода $s_{i}\!\times\!a_{k}~\to~s_{j}$ эти требования можно сформулировать следующим образом:. \todo{Формально это соответствует следующей системе уравнений:}

Данная схема позволяет построить для произволного \acr{FSM} функционально эквивалентную нейросетевую модель. Перед началом моделирования начальное состоние $S_{0}$ может быть задано явно, например, как одно из начальных условий: $\nu_{s_{0}} = \nu^{high}$.

Для наглядности \onfigure~\ref{img:fsm_example_topology}а изображена простейшая схема \acr{FSM} с $\usualset{A} = \{ a_{1}, a_{2} \}$, $\usualset{S} = \{ s_{1}, s_{2}, s_{3} \}$ и функцией $\delta$, определённой как $\{ s_{1}\!\times\!a_{1} \to s_{2}$, $s_{2}\!\times\!a_{1} \to s_{3}$, $s_{3}\!\times\!a_{2} \to s_{1} \}$. Результат применения описанного выше преобразования --- функционально эквивалентная \acr{NN}, изображённая \onfigure~\ref{img:fsm_example_topology}б (на рисунке также изображены рекуррентные связи нейронов $\alpha_{st}$ и $\alpha_{tr}$). Используя параметры сети \todo{параметры}, можно вычислить коэффициенты связей (\seefigure~\ref{img:fsm_example_weights}): \todo{коэффициенты}.

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\textwidth]{fsm_example_topology}}
    \caption{Пример построения нейросетевого \acr{FSM}: а) диаграмма состояний автомата; б) эквивалентная диаграмме состояний схема \acr{NN}.} 
    \label{img:fsm_example_topology}  
\end{figure}

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{fsm_example_weights}}
    \caption{Допустимые области значений для весовых коэффициентов, полученные при использовании параметров: \todo{значения параметров}. Ограничения обусловлены функциональными требованиями, наложенными на различные типы нейронов. } 
    \label{img:fsm_example_weights}  
\end{figure}


\subsubsection{Результаты численных экспериментов}

Результат моделирования простейшей схемы \acr{FSM} из предыдущего подраздела (\seefigure~\ref{img:fsm_example_topology}) представлен \onfigure~\ref{img:fsm_example_dynamic}, на котором отражена частота нейронов \inquotes{состояний} и периоды предъявления входных символов. Видно, что при подаче допустимых входных символов, \ie для которых определён переход из текущего состояния, происходит смена конфигурации: соответствующий текущему состоянию нейрон становится неактивным, а соответствующий новому состоянию --- активным. В то же время, при подаче недопустимого символа (интервал времени $[350; 375]$) состояние сети качественно остаётся без изменений. Кроме того, стоит отметить что непрерывное предъявление входного символа приводит к последовательной смене состояний до тех пор, пока схема автомата это допускает (интервал времени $[450; 500]$), что отражает периодичность происходящих в модели процессов.

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{fsm_example_dynamic}}
    \caption{Динамика нейронов \inquotes{состояний} нейросетевой модели \acr{FSM}, представленной \onfigure~\ref{img:fsm_example_topology}б. Верхние горизонтальные линии отражают временн\'{ы}е периоды считывания входных символов.} 
    \label{img:fsm_example_dynamic}  
\end{figure}

Также работоспособность данного подхода проверялась и на более сложных примерах \acr{FSM}, один из которых приведён \onfigure~\ref{img:fsm_model_topology}, --- стоит отметить наличие в диаграмме состояний циклов и двойных переходов (наличие двойных дуг от одного состояния к другому). Как и ранее, в результате применения описанного выше преобразования была получена функционально эквивалентная нейросетевая модель со следующими параметрами: \todo{параметры}. Различные численные эксперименты, один из которых приведён \onfigure~\ref{img:fsm_model_dynamic}, показали, что нейросетевая модель автомата успешно обрабатывает любую последовательность входных символов, игнорируя недопустимые символы. При этом работоспособность сохраняется и при обработке достаточно длинных последовательностей (максимальная использованная длина последовательности составила 1000 символов) и при наличии больших временн\'{ы}х задержек между предъявлениями символов (максимальная использованная задержка составила порядка 10000 единиц), что подтверждает работоспособность данного подхода.

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{fsm_model_topology}}
    \caption{Диаграмма состояний \acr{FSM}, использованного в процессе моделирования.}
    \label{img:fsm_model_topology}  
\end{figure}

\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{fsm_model_dynamic}}
    \caption{Результаты численного моделирования нейросетевого \acr{FSM}, представленного \onfigure~\ref{img:fsm_model_topology}. Ось ординат соответствует индексам нейронов \inquotes{состояний}. Значение частоты нейронов выражено оттенками серого в соответствии со шкалой справа.} 
    \label{img:fsm_model_dynamic}  
\end{figure}

%==============================================================================
%                               Выводы по главе
%==============================================================================
\section{Выводы по главе \thechapter} \label{section:neuron_concls}



